# reinforcement-learning
a reinforcement learning final project

ğŸš— DeepSpeed
Geometry-Aware Reinforcement Learning for High-Performance Autonomous Racing

Course Project â€” Reinforcement Learning for Robotics
Arizona State University â€” School of Electrical, Computer, and Energy Engineering

Team

DeepSpeed

Kaiyuan Tan

Perfect Obumneme

Emmanuel Adeloju

Sung Park

Atharva Hundare

Contacts: [ktan24, pobumnem, eadeloju, spark259, ahundare]@asu.edu

Abstract

We present DeepSpeed, a geometry-driven reinforcement learning framework for autonomous racing built on the Amazon Web Services AWS DeepRacer platform.

Unlike conventional reward engineering approaches that directly incentivize speed, progress, or on-track heuristics, our method formulates driving as a pure steering alignment problem derived from geometric first principles.

We show that:

Proper steering alignment with the future curvature of the track implicitly produces
speed, stability, and faster lap times.

Thus, DeepSpeed:

âŒ does not reward speed

âŒ does not reward progress

âŒ does not reward wheels-on-track

âœ… rewards only optimal steering decisions

This minimalistic reward design leads to:

smoother trajectories

earlier corner entry

natural apex cutting

improved stability across unseen tracks

Table of Contents

Problem Formulation

Environment

Geometric Foundations

Track Processing

Lookahead Target Selection

Optimal Steering Derivation

Reward Function Design

RL Configuration

Training Procedure

Evaluation Results

Ablation & Insights

Future Work

Repository Structure

Credits

1. Problem Formulation

We cast autonomous racing as a Markov Decision Process (MDP):

ğ‘€
=
(
ğ‘†
,
ğ´
,
ğ‘ƒ
,
ğ‘…
,
ğ›¾
)
M=(S,A,P,R,Î³)

where

$s_t$ = vehicle pose + track observations

$a_t$ = steering, throttle

$R(s_t,a_t)$ = reward

$\gamma$ = discount factor

Goal:

ğœ‹
âˆ—
=
arg
â¡
max
â¡
ğœ‹
ğ¸
[
âˆ‘
ğ‘¡
=
0
ğ‘‡
ğ›¾
ğ‘¡
ğ‘…
(
ğ‘ 
ğ‘¡
,
ğ‘
ğ‘¡
)
]
Ï€
âˆ—
=arg
Ï€
max
	â€‹

E[
t=0
âˆ‘
T
	â€‹

Î³
t
R(s
t
	â€‹

,a
t
	â€‹

)]

Instead of directly maximizing velocity or progress, we optimize future heading alignment.

2. Environment

Platform: AWS DeepRacer simulator

camera-based perception

continuous control

multiple racetracks

time-trial evaluation

Performance metric:

Lap Time (seconds)
â†“
Lap Time (seconds)â†“
3. Geometric Foundations

At each timestep the vehicle solves a local geometric control problem.

State Representation

Car pose:

ğ‘
=
(
ğ‘¥
,
ğ‘¦
)
âˆˆ
ğ‘…
2
,
ğœƒ
âˆˆ
[
âˆ’
ğœ‹
,
ğœ‹
]
p=(x,y)âˆˆR
2
,Î¸âˆˆ[âˆ’Ï€,Ï€]

Track:

ğ‘Š
=
{
ğ‘¤
1
,
ğ‘¤
2
,
â€¦
,
ğ‘¤
ğ‘
}
W={w
1
	â€‹

,w
2
	â€‹

,â€¦,w
N
	â€‹

}
Core Primitives

Our policy depends only on:

Euclidean distance

Polar direction

Shortest rotation angle

4. Track Processing
Problem

Raw waypoints are sparse â†’ straight-line approximations.

This causes:

jagged control

inaccurate curvature estimation

Solution â€” Upsampling

We densify:

ğ‘Š
â†’
ğ‘Š
~
Wâ†’
W
~

using interpolation to approximate a continuous curve.

Benefits:

smoother geometry

stable targets

reduced oscillations

5. Lookahead Target Selection

We introduce a lookahead radius $r$.

Algorithm

Find nearest waypoint

Move forward distance $r$

Select target $w_t$

This predicts future track curvature rather than immediate position.

6. Optimal Steering Derivation

Let

ğ‘‘
âƒ—
=
ğ‘¤
ğ‘¡
âˆ’
ğ‘
d
=w
t
	â€‹

âˆ’p

Target direction:

ğœ™
=
atan2
(
ğ‘‘
ğ‘¦
,
ğ‘‘
ğ‘¥
)
Ï•=atan2(d
y
	â€‹

,d
x
	â€‹

)

Heading error:

Î”
ğœƒ
=
ğœ™
âˆ’
ğœƒ
Î”Î¸=Ï•âˆ’Î¸

Optimal steering:

ğ›¿
âˆ—
=
clip
(
Î”
ğœƒ
)
Î´
âˆ—
=clip(Î”Î¸)

Thus:

Steering becomes pure angle minimization.

7. Reward Function Design
Core Philosophy

We reward only steering correctness.

Final Reward
ğ‘…
=
exp
â¡
(
âˆ’
ğ‘˜
âˆ£
Î”
ğœƒ
âˆ£
)
R=exp(âˆ’kâˆ£Î”Î¸âˆ£)

where:

small error â†’ large reward

large misalignment â†’ exponential penalty

Not incentivized

speed

progress

on-track checks

Why this works

Correct alignment automatically yields:

earlier corner entry

apex cutting

straight exits

higher stable speeds

Hence:

Good steering
â‡’
Good speed
Good steeringâ‡’Good speed
8. RL Configuration
Category	Value
Algorithm	PPO
Framework	TensorFlow
Action space	Continuous
Speed	0.5â€“3.7 m/s
Steering	âˆ’27Â° to 28Â°
Batch size	64
Entropy	0.01
Discount	0.99
Learning rate	3e-4
Epochs	10
9. Training

Training track: Circuit de Barcelona-Catalunya
Training time: ~190 minutes

Procedure:

design reward

train PPO

evaluate on unseen tracks

iterate

10. Evaluation Results

Generalization tested on multiple unseen tracks.

Lap time gap vs world record
Case	Performance
Best	~7% slower
Worst	~32% slower
Observations

stable across tracks

fewer spin-outs

smooth trajectories

strong cornering behavior

11. Key Insight

Most teams trade:

speed â†” off-track penalties

DeepSpeed achieves both by:

optimizing geometry instead of heuristics.

12. Future Work
Corner Stability

Penalize large steering derivatives:

âˆ£
ğ›¿
ğ‘¡
âˆ’
ğ›¿
ğ‘¡
âˆ’
1
âˆ£
âˆ£Î´
t
	â€‹

âˆ’Î´
tâˆ’1
	â€‹

âˆ£
Speed Awareness

Reward speed only when aligned:

ğ‘£
â‹…
cos
â¡
(
Î”
ğœƒ
)
vâ‹…cos(Î”Î¸)
Smooth Acceleration

Throttle regularization

Lap Bonus

Fast completion reward

14. Credits & Contributions
Member	Contribution
Kaiyuan Tan	Model creation + training
Perfect Obumneme	Reward iteration, equations, plots
Emmanuel Adeloju	Multi-track evaluation experiments
Sung Park	Reward tuning + hyperparameters
Atharva Hundare	Final reward design + evaluation
Citation

If you reference this work:

DeepSpeed: Geometry-Aware Reinforcement Learning for Autonomous Racing, ASU EEE RL Project, 2025.

