# reinforcement-learning
a reinforcement learning final project
ğŸš— DeepSpeed
Geometry-Aware Reinforcement Learning for High-Performance Autonomous Racing

Course Project â€” Reinforcement Learning for Robotics
Arizona State University â€” School of Electrical, Computer, and Energy Engineering

ğŸ‘¥ Team

DeepSpeed

Kaiyuan Tan

Perfect Obumneme

Emmanuel Adeloju

Sung Park

Atharva Hundare

ğŸ“§ Contacts:
[ktan24, pobumnem, eadeloju, spark259, ahundare]@asu.edu

ğŸ“Œ Abstract

We present DeepSpeed, a geometry-driven reinforcement learning framework for autonomous racing built on the AWS DeepRacer platform.

Unlike conventional reward engineering approaches that directly incentivize speed, progress, or on-track heuristics, our method formulates driving as a pure steering alignment problem derived from geometric first principles.

We show that:

Proper steering alignment with future track curvature implicitly produces speed, stability, and faster lap times.

DeepSpeed:

âŒ does not reward speed
âŒ does not reward progress
âŒ does not reward wheels-on-track
âœ… rewards only optimal steering decisions

This minimalistic reward design produces:

Smoother trajectories

Earlier corner entry

Natural apex cutting

Improved stability across unseen tracks

ğŸ“š Table of Contents

Problem Formulation

Environment

Geometric Foundations

Track Processing

Lookahead Target Selection

Optimal Steering Derivation

Reward Function Design

RL Configuration

Training

Evaluation Results

Key Insight

Future Work

Credits

1ï¸âƒ£ Problem Formulation

We cast autonomous racing as a Markov Decision Process (MDP):

ğ‘€
=
(
ğ‘†
,
ğ´
,
ğ‘ƒ
,
ğ‘…
,
ğ›¾
)
M=(S,A,P,R,Î³)

Where:

$s_t$ = vehicle pose + track observations

$a_t$ = steering, throttle

$R(s_t, a_t)$ = reward

$\gamma$ = discount factor

Goal:

ğœ‹
âˆ—
=
arg
â¡
max
â¡
ğœ‹
ğ¸
[
âˆ‘
ğ‘¡
=
0
ğ‘‡
ğ›¾
ğ‘¡
ğ‘…
(
ğ‘ 
ğ‘¡
,
ğ‘
ğ‘¡
)
]
Ï€
âˆ—
=arg
Ï€
max
	â€‹

E[
t=0
âˆ‘
T
	â€‹

Î³
t
R(s
t
	â€‹

,a
t
	â€‹

)]

Instead of maximizing velocity or progress directly, we optimize future heading alignment.

2ï¸âƒ£ Environment

Platform: AWS DeepRacer simulator

Camera-based perception

Continuous control

Multiple racetracks

Time-trial evaluation

Performance Metric:

Lap Time (seconds)
â†“
Lap Time (seconds)â†“
3ï¸âƒ£ Geometric Foundations

At each timestep the vehicle solves a local geometric control problem.

State Representation

Car pose:

ğ‘
=
(
ğ‘¥
,
ğ‘¦
)
âˆˆ
ğ‘…
2
,
ğœƒ
âˆˆ
[
âˆ’
ğœ‹
,
ğœ‹
]
p=(x,y)âˆˆR
2
,Î¸âˆˆ[âˆ’Ï€,Ï€]

Track waypoints:

ğ‘Š
=
{
ğ‘¤
1
,
ğ‘¤
2
,
.
.
.
,
ğ‘¤
ğ‘
}
W={w
1
	â€‹

,w
2
	â€‹

,...,w
N
	â€‹

}
Core Primitives

Our policy depends only on:

Euclidean distance

Polar direction

Shortest rotation angle

4ï¸âƒ£ Track Processing
Problem

Raw waypoints are sparse â†’ straight-line approximations.

This causes:

Jagged control

Inaccurate curvature estimation

Solution â€” Upsampling

We densify:

ğ‘Š
â†’
ğ‘Š
~
Wâ†’
W
~

using interpolation to approximate a continuous curve.

Benefits:

Smoother geometry

Stable targets

Reduced oscillations

5ï¸âƒ£ Lookahead Target Selection

We introduce a lookahead radius $r$.

Algorithm

Find nearest waypoint

Move forward distance $r$

Select target $w_t$

This predicts future curvature instead of immediate position.

6ï¸âƒ£ Optimal Steering Derivation

Let:

ğ‘‘
âƒ—
=
ğ‘¤
ğ‘¡
âˆ’
ğ‘
d
=w
t
	â€‹

âˆ’p

Target direction:

ğœ™
=
atan2
(
ğ‘‘
ğ‘¦
,
ğ‘‘
ğ‘¥
)
Ï•=atan2(d
y
	â€‹

,d
x
	â€‹

)

Heading error:

Î”
ğœƒ
=
ğœ™
âˆ’
ğœƒ
Î”Î¸=Ï•âˆ’Î¸

Optimal steering:

ğ›¿
âˆ—
=
clip
(
Î”
ğœƒ
)
Î´
âˆ—
=clip(Î”Î¸)

Steering becomes pure angle minimization.

7ï¸âƒ£ Reward Function Design
Core Philosophy

We reward only steering correctness.

Final Reward
ğ‘…
=
exp
â¡
(
âˆ’
ğ‘˜
âˆ£
Î”
ğœƒ
âˆ£
)
R=exp(âˆ’kâˆ£Î”Î¸âˆ£)

Where:

Small error â†’ large reward

Large misalignment â†’ exponential penalty

Not incentivized

Speed

Progress

On-track checks

Why This Works

Correct alignment naturally yields:

Earlier corner entry

Apex cutting

Straight exits

Higher stable speeds

Good steering â‡’ Good speed

8ï¸âƒ£ RL Configuration
Category	Value
Algorithm	PPO
Framework	TensorFlow
Action Space	Continuous
Speed	0.5â€“3.7 m/s
Steering	âˆ’27Â° to 28Â°
Batch Size	64
Entropy	0.01
Discount	0.99
Learning Rate	3e-4
Epochs	10
9ï¸âƒ£ Training

Training Track: Circuit de Barcelona-Catalunya
Training Time: ~190 minutes

Procedure:

Design reward

Train PPO

Evaluate on unseen tracks

Iterate

ğŸ”Ÿ Evaluation Results

Generalization tested on multiple unseen tracks.

Lap Time Gap vs World Record
Case	Performance
Best	~7% slower
Worst	~32% slower
Observations

Stable across tracks

Fewer spin-outs

Smooth trajectories

Strong cornering behavior

1ï¸âƒ£1ï¸âƒ£ Key Insight

Most teams trade:

speed â†” off-track penalties

DeepSpeed achieves both by optimizing geometry instead of heuristics.

1ï¸âƒ£2ï¸âƒ£ Future Work
Corner Stability
âˆ£
ğ›¿
ğ‘¡
âˆ’
ğ›¿
ğ‘¡
âˆ’
1
âˆ£
âˆ£Î´
t
	â€‹

âˆ’Î´
tâˆ’1
	â€‹

âˆ£
Speed Awareness
ğ‘£
â‹…
cos
â¡
(
Î”
ğœƒ
)
vâ‹…cos(Î”Î¸)
Additional Ideas

Throttle regularization

Fast completion bonus

1ï¸âƒ£3ï¸âƒ£ Credits & Contributions
Member	Contribution
Kaiyuan Tan	Model creation + training
Perfect Obumneme	Reward iteration, equations, plots
Emmanuel Adeloju	Multi-track evaluation experiments
Sung Park	Reward tuning + hyperparameters
Atharva Hundare	Final reward design + evaluation
ğŸ“– Citation

If referencing this work:

DeepSpeed: Geometry-Aware Reinforcement Learning for Autonomous Racing,
ASU EEE RL Project, 2025.

